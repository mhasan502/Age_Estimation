{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aabb63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:37.323158Z",
     "start_time": "2021-08-31T19:49:36.115158Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from parse import parse\n",
    "from autocrop import Cropper\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Scale, Grayscale, Resize, transforms\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from custom_loader import AgeDBDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb64b0db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:37.339158Z",
     "start_time": "2021-08-31T19:49:37.324160Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "num_of_class = 102\n",
    "hidden_unit = 256\n",
    "learning_rate = 1e-04\n",
    "batch_size = 64\n",
    "input_size = 64\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277b6da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:37.355159Z",
     "start_time": "2021-08-31T19:49:37.341158Z"
    }
   },
   "outputs": [],
   "source": [
    "transformA = A.Compose([\n",
    "    A.Resize(input_size, input_size),\n",
    "    A.ToGray(p=1),\n",
    "    A.Rotate(limit=10, p=0.3),\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.OpticalDistortion(p=0.2),\n",
    "    A.augmentations.transforms.ChannelDropout(p=1.0),\n",
    "    A.OneOf([\n",
    "        A.Blur(blur_limit=3, p=0.2),\n",
    "        A.ColorJitter(p=0.2),\n",
    "    ], p=0.2),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e2a987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:37.370158Z",
     "start_time": "2021-08-31T19:49:37.356159Z"
    }
   },
   "outputs": [],
   "source": [
    "### --- AgeDB Dataset Class --- ###           {}_{person}_{age}_{gender}.jpg\n",
    "\n",
    "\n",
    "class AgeDBDataset(Dataset):\n",
    "\n",
    "    ## data loading\n",
    "    def __init__(self, directory, device, transform=None, **kwargs):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "\n",
    "        gender_to_class_id = {'m': 0, 'f': 1}\n",
    "\n",
    "        for i, file in enumerate(sorted(os.listdir(self.directory))):\n",
    "            file_labels = parse('{}_{}_{age}_{gender}.jpg', file)\n",
    "\n",
    "            if file_labels is None:\n",
    "                continue\n",
    "\n",
    "            image = Image.open(os.path.join(self.directory,\n",
    "                                            file)).convert('RGB')\n",
    "\n",
    "            ########\n",
    "            cropper = Cropper()\n",
    "\n",
    "            try:\n",
    "                #Get a Numpy array of the cropped image\n",
    "                cropped_array = cropper.crop(image)\n",
    "                #Save the cropped image with PIL\n",
    "                image = Image.fromarray(cropped_array)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            image = np.array(image)\n",
    "\n",
    "            for i in range(1):\n",
    "                augmented_images = self.transform(image=image)['image']\n",
    "                self.images.append(augmented_images)\n",
    "                ########\n",
    "                gender = gender_to_class_id[file_labels['gender']]\n",
    "                age = int(file_labels['age'])\n",
    "                self.labels.append({\n",
    "                    'age': age,\n",
    "                    'gender': gender\n",
    "                })\n",
    "\n",
    "## len(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "## dataset[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        image = self.images[index]\n",
    "\n",
    "        labels = {\n",
    "            'age': self.labels[index]['age'],\n",
    "            'gender': self.labels[index]['gender']\n",
    "        }\n",
    "\n",
    "        return image.to(self.device), labels\n",
    "\n",
    "\n",
    "## DataLoaders - train, validate, test\n",
    "\n",
    "    def get_loaders(self, batch_size, train_size, test_size, random_seed,\n",
    "                    **kwargs):\n",
    "        train_len = int(len(self) * train_size)\n",
    "        test_len = int(len(self) * test_size)\n",
    "        validate_len = len(self) - (train_len + test_len)\n",
    "\n",
    "        self.trainDataset, self.validateDataset, self.testDataset = torch.utils.data.random_split(\n",
    "            dataset=self,\n",
    "            lengths=[train_len, validate_len, test_len],\n",
    "            generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "        train_loader = DataLoader(self.trainDataset, batch_size=batch_size)\n",
    "        validate_loader = DataLoader(self.validateDataset,\n",
    "                                     batch_size=batch_size)\n",
    "        test_loader = DataLoader(self.testDataset, batch_size=batch_size)\n",
    "\n",
    "        return train_loader, validate_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de83481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:55.547158Z",
     "start_time": "2021-08-31T19:49:37.371159Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = AgeDBDataset(\n",
    "    directory='AgeDB/',\n",
    "    transform=transformA,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64cc1bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:55.562159Z",
     "start_time": "2021-08-31T19:49:55.548159Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set, validation_set, test_set = dataset.get_loaders(\n",
    "    batch_size=batch_size,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f3a0e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:55.578158Z",
     "start_time": "2021-08-31T19:49:55.565159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16488"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5641b0b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:55.610158Z",
     "start_time": "2021-08-31T19:49:55.580161Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code of 'Searching Central Difference Convolutional Networks for Face Anti-Spoofing' \n",
    "By Zitong Yu & Zhuo Su, 2019\n",
    "\n",
    "If you use the code, please cite:\n",
    "@inproceedings{yu2020searching,\n",
    "    title={Searching Central Difference Convolutional Networks for Face Anti-Spoofing},\n",
    "    author={Yu, Zitong and Zhao, Chenxu and Wang, Zezheng and Qin, Yunxiao and Su, Zhuo and Li, Xiaobai and Zhou, Feng and Zhao, Guoying},\n",
    "    booktitle= {CVPR},\n",
    "    year = {2020}\n",
    "}\n",
    "\n",
    "Only for research purpose, and commercial use is not allowed.\n",
    "\n",
    "MIT License\n",
    "Copyright (c) 2020 \n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "########################   Centeral-difference (second order, with 9 parameters and a const theta for 3x3 kernel) 2D Convolution   ##############################\n",
    "## | a1 a2 a3 |   | w1 w2 w3 |\n",
    "## | a4 a5 a6 | * | w4 w5 w6 | --> output = \\sum_{i=1}^{9}(ai * wi) - \\sum_{i=1}^{9}wi * a5 --> Conv2d (k=3) - Conv2d (k=1)\n",
    "## | a7 a8 a9 |   | w7 w8 w9 |\n",
    "##\n",
    "##   --> output = \n",
    "## | a1 a2 a3 |   |  w1  w2  w3 |     \n",
    "## | a4 a5 a6 | * |  w4  w5  w6 |  -  | a | * | w\\_sum |     (kernel_size=1x1, padding=0)\n",
    "## | a7 a8 a9 |   |  w7  w8  w9 |     \n",
    "\n",
    "class Conv2d_cd(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.7):\n",
    "\n",
    "        super(Conv2d_cd, self).__init__() \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "\n",
    "        if math.fabs(self.theta - 0.0) < 1e-8:\n",
    "            return out_normal \n",
    "        else:\n",
    "            #pdb.set_trace()\n",
    "            [C_out,C_in, kernel_size,kernel_size] = self.conv.weight.shape\n",
    "            kernel_diff = self.conv.weight.sum(2).sum(2)\n",
    "            kernel_diff = kernel_diff[:, :, None, None]\n",
    "            out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=0, groups=self.conv.groups)\n",
    "\n",
    "            return out_normal - self.theta * out_diff\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel = 3):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=kernel, padding=kernel//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "class CDCN(nn.Module):\n",
    "\n",
    "    def __init__(self, basic_conv=Conv2d_cd, theta=0.7):   \n",
    "        super(CDCN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            basic_conv(3, 64, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "        self.Block1 = nn.Sequential(\n",
    "            basic_conv(64, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.Block2 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.Block3 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.lastconv1 = nn.Sequential(\n",
    "            basic_conv(128*3, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "        self.lastconv2 = nn.Sequential(\n",
    "            basic_conv(128, 64, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "        self.lastconv3 = nn.Sequential(\n",
    "            basic_conv(64, 1, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.downsample32x32 = nn.Upsample(size=(32, 32), mode='bilinear')\n",
    "\n",
    " \n",
    "    def forward(self, x):\t    \t# x [3, 256, 256]\n",
    "        \n",
    "        x_input = x\n",
    "        x = self.conv1(x)\t\t   \n",
    "        \n",
    "        x_Block1 = self.Block1(x)\t    \t    \t# x [128, 128, 128]\n",
    "        x_Block1_32x32 = self.downsample32x32(x_Block1)   # x [128, 32, 32]  \n",
    "        \n",
    "        x_Block2 = self.Block2(x_Block1)\t    # x [128, 64, 64]\t  \n",
    "        x_Block2_32x32 = self.downsample32x32(x_Block2)   # x [128, 32, 32]  \n",
    "        \n",
    "        x_Block3 = self.Block3(x_Block2)\t    # x [128, 32, 32]  \t\n",
    "        x_Block3_32x32 = self.downsample32x32(x_Block3)   # x [128, 32, 32]  \n",
    "        \n",
    "        x_concat = torch.cat((x_Block1_32x32,x_Block2_32x32,x_Block3_32x32), dim=1)    # x [128*3, 32, 32]  \n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        x = self.lastconv1(x_concat)    # x [128, 32, 32] \n",
    "        x = self.lastconv2(x)    # x [64, 32, 32] \n",
    "        x = self.lastconv3(x)    # x [1, 32, 32] \n",
    "        \n",
    "        map_x = x.squeeze(1)\n",
    "        \n",
    "        return map_x, x_concat, x_Block1, x_Block2, x_Block3, x_input\n",
    "\n",
    "\t\t\n",
    "\n",
    "class CDCNpp(nn.Module):\n",
    "\n",
    "    def __init__(self, basic_conv=Conv2d_cd, theta=0.0):   \n",
    "        super(CDCNpp, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            basic_conv(3, 64, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),    \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.Block1 = nn.Sequential(\n",
    "            basic_conv(64, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            basic_conv(128, int(128*1.6), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.6)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.6), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.Block2 = nn.Sequential(\n",
    "            basic_conv(128, int(128*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.2)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.2), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(128, int(128*1.4), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.4)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.4), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.Block3 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            basic_conv(128, int(128*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.2)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.2), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        # Original\n",
    "        \n",
    "        self.lastconv1 = nn.Sequential(\n",
    "            basic_conv(128*3, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            basic_conv(128, 1, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.sa1 = SpatialAttention(kernel = 7)\n",
    "        self.sa2 = SpatialAttention(kernel = 5)\n",
    "        self.sa3 = SpatialAttention(kernel = 3)\n",
    "        self.downsample32x32 = nn.Upsample(size=(32, 32), mode='bilinear')\n",
    "\n",
    " \n",
    "    def forward(self, x):\t    \t# x [3, 256, 256]\n",
    "        \n",
    "        x_input = x\n",
    "        x = self.conv1(x)\t\t   \n",
    "        \n",
    "        x_Block1 = self.Block1(x)\t    \t    \t\n",
    "        attention1 = self.sa1(x_Block1)\n",
    "        x_Block1_SA = attention1 * x_Block1\n",
    "        x_Block1_32x32 = self.downsample32x32(x_Block1_SA)   \n",
    "        \n",
    "        x_Block2 = self.Block2(x_Block1)\t    \n",
    "        attention2 = self.sa2(x_Block2)  \n",
    "        x_Block2_SA = attention2 * x_Block2\n",
    "        x_Block2_32x32 = self.downsample32x32(x_Block2_SA)  \n",
    "        \n",
    "        x_Block3 = self.Block3(x_Block2)\t    \n",
    "        attention3 = self.sa3(x_Block3)  \n",
    "        x_Block3_SA = attention3 * x_Block3\t\n",
    "        x_Block3_32x32 = self.downsample32x32(x_Block3_SA)   \n",
    "        \n",
    "        x_concat = torch.cat((x_Block1_32x32,x_Block2_32x32,x_Block3_32x32), dim=1)    \n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        map_x = self.lastconv1(x_concat)\n",
    "        \n",
    "        map_x = map_x.squeeze(1)\n",
    "        \n",
    "        return map_x, x_concat, attention1, attention2, attention3, x_input\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6938c26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:57.383157Z",
     "start_time": "2021-08-31T19:49:55.612158Z"
    }
   },
   "outputs": [],
   "source": [
    "convModel = CDCNpp().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed7d490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:57.399160Z",
     "start_time": "2021-08-31T19:49:57.384157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, optimizer, criterion, train_loader, num_of_epoch):\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_of_epoch):\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device).float()\n",
    "            labels = torch.as_tensor(labels['age']).to(device).float()\n",
    "            \n",
    "            outputs, _, _, _, _, _ = model(imgs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1)%total_step == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{num_of_epoch}, Step: {i+1}/{total_step}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca9b2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:57.415158Z",
     "start_time": "2021-08-31T19:49:57.400158Z"
    }
   },
   "outputs": [],
   "source": [
    "def contrast_depth_conv(input, device):\n",
    "    ''' compute contrast depth in both of (out, label) '''\n",
    "    '''\n",
    "        input  32x32\n",
    "        output 8x32x32\n",
    "    '''\n",
    "\n",
    "    kernel_filter_list =[\n",
    "                        [[1,0,0],[0,-1,0],[0,0,0]], [[0,1,0],[0,-1,0],[0,0,0]], [[0,0,1],[0,-1,0],[0,0,0]],\n",
    "                        [[0,0,0],[1,-1,0],[0,0,0]], [[0,0,0],[0,-1,1],[0,0,0]],\n",
    "                        [[0,0,0],[0,-1,0],[1,0,0]], [[0,0,0],[0,-1,0],[0,1,0]], [[0,0,0],[0,-1,0],[0,0,1]]\n",
    "                        ]\n",
    "    \n",
    "    kernel_filter = np.array(kernel_filter_list, np.float32)\n",
    "    \n",
    "    kernel_filter = torch.from_numpy(kernel_filter.astype(np.float)).float().to(device)\n",
    "    # weights (in_channel, out_channel, kernel, kernel)\n",
    "    kernel_filter = kernel_filter.unsqueeze(dim=1)\n",
    "    \n",
    "    input = input.unsqueeze(dim=1).expand(input.shape[0], 8, input.shape[1],input.shape[2])\n",
    "    \n",
    "    contrast_depth = F.conv2d(input, weight=kernel_filter, groups=8)  # depthwise conv\n",
    "    \n",
    "    return contrast_depth\n",
    "\n",
    "\n",
    "class ContrastDepthLoss(nn.Module):    # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n",
    "    def __init__(self, device):\n",
    "        super(ContrastDepthLoss, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, out, label): \n",
    "        '''\n",
    "        compute contrast depth in both of (out, label),\n",
    "        then get the loss of them\n",
    "        tf.atrous_convd match tf-versions: 1.4\n",
    "        '''\n",
    "        contrast_out = contrast_depth_conv(out, device=self.device)\n",
    "        contrast_label = contrast_depth_conv(label, device=self.device)\n",
    "\n",
    "        criterion_MSE = nn.MSELoss()\n",
    "    \n",
    "        loss = criterion_MSE(contrast_out, contrast_label)\n",
    "    \n",
    "        return loss\n",
    "\n",
    "\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(DepthLoss, self).__init__()\n",
    "        self.criterion_absolute_loss = nn.MSELoss()\n",
    "        self.criterion_contrastive_loss = ContrastDepthLoss(device=device)\n",
    "\n",
    "\n",
    "    def forward(self, predicted_depth_map, gt_depth_map):\n",
    "        absolute_loss = self.criterion_absolute_loss(predicted_depth_map, gt_depth_map)\n",
    "        contrastive_loss = self.criterion_contrastive_loss(predicted_depth_map, gt_depth_map)\n",
    "        return absolute_loss + contrastive_loss\n",
    "    \n",
    "criteria = ContrastDepthLoss(device=device) \n",
    "optimizer = torch.optim.Adam(convModel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed4bb3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:59.176159Z",
     "start_time": "2021-08-31T19:49:57.416159Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f160dbaa5765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-2ada184cffad>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_loader, num_of_epoch)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0eb591c3978f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, out, label)\u001b[0m\n\u001b[0;32m     38\u001b[0m         '''\n\u001b[0;32m     39\u001b[0m         \u001b[0mcontrast_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrast_depth_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mcontrast_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrast_depth_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mcriterion_MSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0eb591c3978f>\u001b[0m in \u001b[0;36mcontrast_depth_conv\u001b[1;34m(input, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mkernel_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcontrast_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# depthwise conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train(convModel, optimizer, criteria, train_set, num_of_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f1efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:59.182159Z",
     "start_time": "2021-08-31T19:49:36.109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def eval(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        error = torch.zeros(0).to(device)\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device).float()\n",
    "            labels = torch.as_tensor(labels['age']).to(device)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            error = torch.cat([error, torch.abs(\n",
    "                torch.subtract(torch.reshape(labels, (-1,)), torch.reshape(pred, (-1,)))\n",
    "            )])\n",
    "                        \n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    print(f\"Accuracy: {(100*correct)/total}%\")\n",
    "    print(f\"Mean Absolute Error: {(torch.mean(error))}\")\n",
    "    print(f\"Minimum: {torch.min(error)}, Maximum: {torch.max(error)}, Median: {torch.median(error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b735ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:59.183158Z",
     "start_time": "2021-08-31T19:49:36.110Z"
    }
   },
   "outputs": [],
   "source": [
    "eval(convModel, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5fe78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T19:49:59.184158Z",
     "start_time": "2021-08-31T19:49:36.110Z"
    }
   },
   "outputs": [],
   "source": [
    "eval(convModel, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
