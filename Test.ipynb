{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aabb63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.083508Z",
     "start_time": "2021-09-13T11:19:08.199510Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from parse import parse\n",
    "import pretrainedmodels\n",
    "from kornia.losses import FocalLoss\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor, Compose, Scale, Grayscale, Resize, transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3904614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.099508Z",
     "start_time": "2021-09-13T11:19:10.085509Z"
    },
    "code_folding": [
     1,
     22,
     94,
     99
    ]
   },
   "outputs": [],
   "source": [
    "from kornia.losses import FocalLoss as focal_loss\n",
    "def label_smoothing_criterion(alpha=0.1, distribution='uniform', std=0.5, reduction='mean'):\n",
    "    def _label_smoothing_criterion(logits, labels):\n",
    "        n_classes = logits.size(1)\n",
    "        device = logits.device\n",
    "        # manipulate labels\n",
    "        one_hot = one_hot_encoding(labels, n_classes).float().to(device)\n",
    "        if distribution == 'uniform':\n",
    "            uniform = torch.ones_like(one_hot).to(device)/n_classes\n",
    "            soft_labels = (1 - alpha)*one_hot + alpha*uniform\n",
    "        elif distribution == 'gaussian':\n",
    "            dist = get_gaussian_label_distribution(n_classes, std=std)\n",
    "            soft_labels = torch.from_numpy(dist[labels.cpu().numpy()]).to(device)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        loss = cross_entropy_loss_one_hot(logits, soft_labels.float(), reduction)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return _label_smoothing_criterion\n",
    "\n",
    "def cost_sensitive_loss(input, target, M):\n",
    "    if input.size(0) != target.size(0):\n",
    "        raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
    "                         .format(input.size(0), target.size(0)))\n",
    "    device = input.device\n",
    "    M = M.to(device)\n",
    "    return (M[target, :]*input.float()).sum(axis=-1)\n",
    "    # return torch.diag(torch.matmul(input, M[:, target]))\n",
    "\n",
    "class CostSensitiveRegularizedLoss(nn.Module):\n",
    "    def __init__(self,  n_classes=5, exp=2, normalization='softmax', reduction='mean', base_loss='ce', lambd=10):\n",
    "        super(CostSensitiveRegularizedLoss, self).__init__()\n",
    "        if normalization == 'softmax':\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        elif normalization == 'sigmoid':\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = None\n",
    "        self.reduction = reduction\n",
    "        x = np.abs(np.arange(n_classes, dtype=np.float32))\n",
    "        M = np.abs((x[:, np.newaxis] - x[np.newaxis, :])) ** exp\n",
    "        #\n",
    "        # M_oph = np.array([\n",
    "        #                 [1469, 4, 5,  0,  0],\n",
    "        #                 [58, 62,  5,  0,  0],\n",
    "        #                 [22, 3, 118,  1,  0],\n",
    "        #                 [0, 0,   13, 36,  1],\n",
    "        #                 [0, 0,    0,  1, 15]\n",
    "        #                 ], dtype=np.float)\n",
    "        # M_oph = M_oph.T\n",
    "        # # Normalize M_oph to obtain M_difficulty:\n",
    "        # M_difficulty = 1-np.divide(M_oph, np.sum(M_oph, axis=1)[:, None])\n",
    "        # # OPTION 1: average M and M_difficulty:\n",
    "        # M = 0.5 * M + 0.5 * M_difficulty\n",
    "        # ################\n",
    "        # # OPTION 2: replace uninformative entries in M_difficulty by entries of M:\n",
    "        # # M_difficulty[M_oph == 0] = M[M_oph == 0]\n",
    "        # # M = M_difficulty\n",
    "\n",
    "        M /= M.max()\n",
    "        self.M = torch.from_numpy(M)\n",
    "        self.lambd = lambd\n",
    "        self.base_loss = base_loss\n",
    "\n",
    "        if self.base_loss == 'ce':\n",
    "            self.base_loss = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "        elif self.base_loss == 'ls':\n",
    "            self.base_loss = label_smoothing_criterion(distribution='uniform', reduction=reduction)\n",
    "        elif self.base_loss == 'gls':\n",
    "            self.base_loss = label_smoothing_criterion(distribution='gaussian', reduction=reduction)\n",
    "        elif self.base_loss == 'focal_loss':\n",
    "            kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": reduction}\n",
    "            self.base_loss = focal_loss(**kwargs)\n",
    "        else:\n",
    "            sys.exit('not a supported base_loss')\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        base_l = self.base_loss(logits, target)\n",
    "        if self.lambd == 0:\n",
    "            return self.base_loss(logits, target)\n",
    "        else:\n",
    "            preds = self.normalization(logits)\n",
    "            loss = cost_sensitive_loss(preds, target, self.M)\n",
    "            if self.reduction == 'none':\n",
    "                return base_l + self.lambd*loss\n",
    "            elif self.reduction == 'mean':\n",
    "                return base_l + self.lambd*loss.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                return base_l + self.lambd*loss.sum()\n",
    "            else:\n",
    "                raise ValueError('`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n",
    "\n",
    "def get_cost_sensitive_criterion(n_classes=5, exp=2):\n",
    "    train_criterion = CostSensitiveLoss(n_classes, exp=exp, normalization='softmax')\n",
    "    val_criterion = CostSensitiveLoss(n_classes, exp=exp, normalization='softmax')\n",
    "    return train_criterion, val_criterion\n",
    "\n",
    "def get_cost_sensitive_regularized_criterion(base_loss='ce', n_classes=5, lambd=1, exp=2):\n",
    "    train_criterion = CostSensitiveRegularizedLoss(n_classes, exp=exp, normalization='softmax', base_loss=base_loss, lambd=lambd)\n",
    "    val_criterion = CostSensitiveRegularizedLoss(n_classes, exp=exp, normalization='softmax', base_loss=base_loss, lambd=lambd)\n",
    "\n",
    "    return train_criterion, val_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb64b0db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.115508Z",
     "start_time": "2021-09-13T11:19:10.101508Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "num_of_class = 102\n",
    "learning_rate = 1e-03\n",
    "batch_size = 128\n",
    "input_size = 224\n",
    "device = torch.device(\"cuda\")\n",
    "train_size = 0.7\n",
    "test_size = 0.2\n",
    "directoryAgeDB = 'AgeDB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef10586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.641518Z",
     "start_time": "2021-09-13T11:19:10.116511Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def imageList():\n",
    "    image_list = []\n",
    "    for i, file in enumerate(sorted(os.listdir(directoryAgeDB))):\n",
    "        file_labels = parse('{}_{person}_{age}_{gender}.jpg', file)\n",
    "\n",
    "        if file_labels is None:\n",
    "            continue\n",
    "\n",
    "        image_location = os.path.join(directoryAgeDB, file)\n",
    "        gender_to_class_id = {'m': 0, 'f': 1}\n",
    "        gender = gender_to_class_id[file_labels['gender']]\n",
    "        age = int(file_labels['age'])\n",
    "        image_list.append({\n",
    "            'image_location': image_location,\n",
    "            'age': age,\n",
    "            'gender': gender\n",
    "        })\n",
    "\n",
    "    return image_list\n",
    "\n",
    "image_list = imageList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e74fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.657508Z",
     "start_time": "2021-09-13T11:19:10.642514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11541\n",
      "3297\n",
      "1650\n"
     ]
    }
   ],
   "source": [
    "train_len = int(len(image_list) * train_size)\n",
    "test_len = int(len(image_list) * test_size)\n",
    "validate_len = len(image_list) - (train_len + test_len)\n",
    "\n",
    "train_image_list, test_image_list, validate_image_list = torch.utils.data.random_split(\n",
    "    dataset = image_list,\n",
    "    lengths = [train_len, test_len, validate_len], \n",
    "    generator = torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(len(train_image_list))\n",
    "print(len(test_image_list))\n",
    "print(len(validate_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31b1aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.672509Z",
     "start_time": "2021-09-13T11:19:10.659509Z"
    },
    "code_folding": [
     36,
     38,
     43
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class AgeDBDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_list, device, train = True, train_transform=None, test_transform=None, **kwargs):\n",
    "        self.image_list = image_list\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "\n",
    "        if self.train:\n",
    "            for i in tqdm(range(len(image_list))):\n",
    "\n",
    "                image = Image.open(self.image_list[i]['image_location']).convert('RGB')\n",
    "                image_location = self.image_list[i]['image_location']\n",
    "                age = self.image_list[i]['age']\n",
    "                gender = self.image_list[i]['gender']\n",
    "\n",
    "                image = np.array(image)\n",
    "\n",
    "                for j in range(1):\n",
    "                    if j == 0:\n",
    "                        augmented_images = self.test_transform(\n",
    "                            image=image)['image']\n",
    "                    else:\n",
    "                        augmented_images = self.train_transform(\n",
    "                            image=image)['image']\n",
    "\n",
    "                    self.images.append(augmented_images)\n",
    "                    self.labels.append({\n",
    "                        'image_location': image_location,\n",
    "                        'age': age,\n",
    "                        'gender': gender\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        if self.train:\n",
    "            return len(self.labels)\n",
    "        else:\n",
    "            return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        if self.train:\n",
    "            image = self.images[index]\n",
    "            labels = {\n",
    "                'image_location': self.labels[index]['image_location'],\n",
    "                'age': self.labels[index]['age'],\n",
    "                'gender': self.labels[index]['gender']\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            image = Image.open(\n",
    "                self.image_list[index]['image_location']).convert('RGB')\n",
    "            image = np.array(image)\n",
    "            image = self.test_transform(image=image)['image']\n",
    "            labels = {\n",
    "                'image_location': self.image_list[index]['image_location'],\n",
    "                'age': self.image_list[index]['age'],\n",
    "                'gender': self.image_list[index]['gender']\n",
    "            }\n",
    "\n",
    "        return image.to(self.device), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41a26d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:10.688507Z",
     "start_time": "2021-09-13T11:19:10.673508Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(input_size, input_size),\n",
    "    A.ToGray(p=1),\n",
    "    A.Rotate(limit=10, p=0.3),\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.OpticalDistortion(p=0.2),\n",
    "    A.OneOf([\n",
    "        A.Blur(blur_limit=3, p=0.2),\n",
    "        A.ColorJitter(p=0.2),\n",
    "    ], p=0.2),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [A.Resize(input_size, input_size),\n",
    "     A.ToGray(p=1),\n",
    "     ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb2dd1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:24.342508Z",
     "start_time": "2021-09-13T11:19:10.689511Z"
    },
    "code_folding": [
     0,
     5,
     10
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11541/11541 [00:13<00:00, 845.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11541\n",
      "3297\n",
      "1650\n",
      "16488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset = AgeDBDataset(image_list=train_image_list,\n",
    "                            device=device,\n",
    "                            train=True,\n",
    "                            train_transform=train_transform,\n",
    "                            test_transform=test_transform)\n",
    "testDataset = AgeDBDataset(image_list=test_image_list,\n",
    "                           device=device,\n",
    "                           train=False,\n",
    "                           train_transform=train_transform,\n",
    "                           test_transform=test_transform)\n",
    "valDataset = AgeDBDataset(image_list=validate_image_list,\n",
    "                          device=device,\n",
    "                          train=False,\n",
    "                          train_transform=train_transform,\n",
    "                          test_transform=test_transform)\n",
    "print(len(trainDataset))\n",
    "print(len(testDataset))\n",
    "print(len(valDataset))\n",
    "print(len(trainDataset) + len(testDataset) + len(valDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e94489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:24.357507Z",
     "start_time": "2021-09-13T11:19:24.343511Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#### --- Oversampling --- ###\n",
    "def sam_weights(TDataset):\n",
    "    listofzeros = [0] * 102\n",
    "    same_age = [i for i in range(0, 102)]\n",
    "    class_weights = dict(zip(same_age, listofzeros))\n",
    "\n",
    "    for i in range(len(TDataset)):\n",
    "        class_weights[TDataset[i][1]['age']] += 1\n",
    "\n",
    "    for i in range(len(class_weights)):\n",
    "        if class_weights[i] > 0:\n",
    "            class_weights[i] = (1 / class_weights[i])\n",
    "\n",
    "    sample_weights = {}\n",
    "\n",
    "    for i in range(len(TDataset)):\n",
    "        sample_weights[i] = class_weights[TDataset[i][1]['age']]\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4b560b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:27.515511Z",
     "start_time": "2021-09-13T11:19:24.358509Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_weights = sam_weights(trainDataset)\n",
    "sample_weights = list(sample_weights.values())\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ae2b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:27.531510Z",
     "start_time": "2021-09-13T11:19:27.516511Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(testDataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "validation_loader = DataLoader(valDataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5641b0b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:27.832507Z",
     "start_time": "2021-09-13T11:19:27.532508Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ResNet18 = pretrainedmodels.resnet18(pretrained='imagenet')\n",
    "ResNet18.last_linear = nn.Linear(\n",
    "    in_features=ResNet18.last_linear.in_features, \n",
    "    out_features=num_of_class, \n",
    "    bias=False\n",
    ")\n",
    "resnetModel = ResNet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed7d490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:27.848507Z",
     "start_time": "2021-09-13T11:19:27.833507Z"
    },
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, optimizer, criterion, train_loader, valid_loader, num_of_epoch):\n",
    "    total_step = len(train_loader)\n",
    "    min_valid_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_of_epoch):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device).float()\n",
    "            labels = torch.as_tensor(labels['age']).to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        for i, (imgs, labels) in enumerate(valid_loader):\n",
    "            with torch.no_grad():\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = torch.as_tensor(labels['age']).to(device)\n",
    "            \n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}/{num_of_epoch}, Train Loss: {train_loss/len(train_loader)},  Validation Loss: {valid_loss/len(valid_loader)}\")\n",
    "              \n",
    "        if min_valid_loss > (valid_loss/len(valid_loader)):\n",
    "            print(f'Validation Loss Decreased({min_valid_loss} ---> {valid_loss/len(valid_loader)})')\n",
    "            min_valid_loss = valid_loss/len(valid_loader)\n",
    "            torch.save(resnetModel.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca9b2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:19:27.864507Z",
     "start_time": "2021-09-13T11:19:27.849509Z"
    }
   },
   "outputs": [],
   "source": [
    "#criteria = FocalLoss(alpha=0.5, gamma=3.0, reduction='mean')\n",
    "criteria = CostSensitiveRegularizedLoss(n_classes=num_of_class, base_loss='focal_loss', reduction='mean') \n",
    "optimizer = torch.optim.SGD(resnetModel.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed4bb3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:23:00.687526Z",
     "start_time": "2021-09-13T11:19:27.865508Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 3.0962836690001434,  Validation Loss: 2.814979443183312\n",
      "Validation Loss Decreased(inf ---> 2.814979443183312)\n",
      "Epoch: 2/10, Train Loss: 2.7133998477851953,  Validation Loss: 2.656817472898043\n",
      "Validation Loss Decreased(2.814979443183312 ---> 2.656817472898043)\n",
      "Epoch: 3/10, Train Loss: 2.6024273987654802,  Validation Loss: 2.5898914337158203\n",
      "Validation Loss Decreased(2.656817472898043 ---> 2.5898914337158203)\n",
      "Epoch: 4/10, Train Loss: 2.5404602459498813,  Validation Loss: 2.542136229001559\n",
      "Validation Loss Decreased(2.5898914337158203 ---> 2.542136229001559)\n",
      "Epoch: 5/10, Train Loss: 2.4868528895325714,  Validation Loss: 2.4961320620316725\n",
      "Validation Loss Decreased(2.542136229001559 ---> 2.4961320620316725)\n",
      "Epoch: 6/10, Train Loss: 2.4322169036655636,  Validation Loss: 2.4515050741342397\n",
      "Validation Loss Decreased(2.4961320620316725 ---> 2.4515050741342397)\n",
      "Epoch: 7/10, Train Loss: 2.3812340589670034,  Validation Loss: 2.405780076980591\n",
      "Validation Loss Decreased(2.4515050741342397 ---> 2.405780076980591)\n",
      "Epoch: 8/10, Train Loss: 2.328433550321139,  Validation Loss: 2.3619563029362607\n",
      "Validation Loss Decreased(2.405780076980591 ---> 2.3619563029362607)\n",
      "Epoch: 9/10, Train Loss: 2.2799417736766103,  Validation Loss: 2.3214781284332275\n",
      "Validation Loss Decreased(2.3619563029362607 ---> 2.3214781284332275)\n",
      "Epoch: 10/10, Train Loss: 2.2373908823663062,  Validation Loss: 2.285564220868624\n",
      "Validation Loss Decreased(2.3214781284332275 ---> 2.285564220868624)\n"
     ]
    }
   ],
   "source": [
    "train(resnetModel, optimizer, criteria, train_loader, validation_loader, num_of_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42f1efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:23:00.702526Z",
     "start_time": "2021-09-13T11:23:00.688527Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def eval(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        error = torch.zeros(0).to(device)\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device).float()\n",
    "            labels = torch.as_tensor(labels['age']).to(device)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            error = torch.cat([error, torch.abs(\n",
    "                torch.subtract(torch.reshape(labels, (-1,)), torch.reshape(pred, (-1,)))\n",
    "            )])\n",
    "                        \n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    #print(f\"Accuracy: {(100*correct)/total}%\")\n",
    "    print(f\"Mean Absolute Error: {(torch.mean(error))}\")\n",
    "    print(f\"Minimum: {torch.min(error)}, Maximum: {torch.max(error)}, Median: {torch.median(error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b735ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:23:11.644529Z",
     "start_time": "2021-09-13T11:23:00.703528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 10.510160446166992\n",
      "Minimum: 0.0, Maximum: 53.0, Median: 9.0\n",
      "Mean Absolute Error: 10.23472785949707\n",
      "Minimum: 0.0, Maximum: 52.0, Median: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(resnetModel, test_loader)\n",
    "eval(resnetModel, train_loader)\n",
    "resnetModel.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb5fd4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:23:15.876528Z",
     "start_time": "2021-09-13T11:23:11.647528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 10.510160446166992\n",
      "Minimum: 0.0, Maximum: 53.0, Median: 9.0\n"
     ]
    }
   ],
   "source": [
    "eval(resnetModel, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9344ede",
   "metadata": {},
   "source": [
    "Mean Absolute Error: 7.715802192687988\n",
    "Minimum: 0.0, Maximum: 51.0, Median: 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d5fe78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:23:22.305527Z",
     "start_time": "2021-09-13T11:23:15.877528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 10.164977073669434\n",
      "Minimum: 0.0, Maximum: 52.0, Median: 8.0\n"
     ]
    }
   ],
   "source": [
    "eval(resnetModel, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdce591",
   "metadata": {},
   "source": [
    "Mean Absolute Error: 6.060046672821045\n",
    "Minimum: 0.0, Maximum: 59.0, Median: 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c02c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
